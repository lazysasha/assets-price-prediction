{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure we run on Tensowflow 2:\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and prepare dataset\n",
    "First, we start with defining helper functions to plot time series and reading data from csv:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None, title=\"Asset price history\", legend=[]):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend(legend)\n",
    "    plt.grid(True)\n",
    "\n",
    "def read_data(filepath='../data/BTC-USD.csv'):\n",
    "    DATE_FORMAT = \"%Y-%m-%d\"\n",
    "    time_step = []\n",
    "    prices = []\n",
    "    with open(filepath) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            if row[4] != 'null':\n",
    "                time_step.append(datetime.strptime(row[0], DATE_FORMAT).date())\n",
    "                prices.append(float(row[4]))\n",
    "    return np.array(time_step), np.array(prices, dtype=\"float32\")\n",
    "\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "time, series = # TODO: call read_data() function with the path to your dataset as an argument\n",
    "### END CODE HERE ###\n",
    "\n",
    "assert not np.any(np.isnan(series))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the time series, let's split it into a training and validation set, so we can start forecasting:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_time = int(len(time) * 0.8)\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_train, x_train, title=\"Training set\")\n",
    "plt.show()\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, title=\"Validation set\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset for training on NN\n",
    "Having dataset loaded in, we have to split it into smaller windows suited for training.\n",
    "Windows will be shuffled and grouped in batches for parallel training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "# EXERCISE 1: Prepare dataset for training\n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "max_train = # TODO: find the max value in `x_train` array\n",
    "            # Hint: use np.max function() https://numpy.org/doc/stable/reference/generated/numpy.maximum.html\n",
    "            # Hint: to be completely sure we do not get negative values, use `np.abs(my_var)` to get absolute value of the variable\n",
    "x_train     # TODO: normalize values in `x_train` by dividing them by `max_train`\n",
    "max_valid = # TODO: find the max value in `x_valid` array\n",
    "            # Hint: use np.max function() https://numpy.org/doc/stable/reference/generated/numpy.maximum.html\n",
    "            # Hint: to be completely sure we do not get negative values, use `np.abs(my_var)` to get absolute value of the variable\n",
    "x_valid # TODO: normalize values in `x_valid` by dividing them by `max_valid`\n",
    "### END CODE HERE ###\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "print(\"First dataset entry: \" + str(list(dataset)[:1]))\n",
    "# If implemented correctly, you should see the first dataset entry with batches of windows of 20 items each, similar to this:\n",
    "### First dataset entry: [(<tf.Tensor: shape=(32, 20), dtype=float32, numpy=\n",
    "### array([[0.01467832, 0.01509622, 0.01453266, 0.0148947 , 0.01409398,\n",
    "###         0.01362541, 0.01373496, 0.01158416, 0.00913471, 0.01076267,\n",
    "###         0.01067306, 0.01021982, 0.01078805, 0.01101998, 0.01083811,\n",
    "###         0.0116373 , 0.01197113, 0.0119441 , 0.0127118 , 0.01301291],"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the model\n",
    "Now when data is prepared we can start creating a NN model.\n",
    "We will implement a simple 3-layer model with 100-10-1 neurons in each layer respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EXERCISE 2: Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # TODO: create a Dense layer with 100 neurons in it. Provide the `[window_size]` as `input_shape` param and use 'relu' as `activation` param\n",
    "    # TODO: create a Dense layer with 10 neurons in ut. Set activation to 'relu'\n",
    "    # TODO: create a Dense layer with 10 neurons in ut. Do not provide the activation function param (use default)\n",
    "    ### END CODE HERE ###\n",
    "])\n",
    "model.summary()\n",
    "# You should see a similar model summary printed:\n",
    "### Model: \"sequential\"\n",
    "### _________________________________________________________________\n",
    "### Layer (type)                 Output Shape              Param #\n",
    "### =================================================================\n",
    "### dense (Dense)                (None, 100)               2100\n",
    "### _________________________________________________________________\n",
    "### dense_1 (Dense)              (None, 10)                1010\n",
    "### _________________________________________________________________\n",
    "### dense_2 (Dense)              (None, 1)                 11\n",
    "### =================================================================\n",
    "### Total params: 3,121\n",
    "### Trainable params: 3,121\n",
    "### Non-trainable params: 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EXERCISE 3: Train the model\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "model.compile(YOUR_CODE_HERE)           # TODO: compile the model. Specify \"mse\" as `loss` param and use `tf.keras.optimizers.SGD` as `optimizer` param\n",
    "                                        # set SGD params `lr` to 1e-6 and `momentum` to 0.9\n",
    "history = model.fit(YOUR_CODE_HERE)     # TODO: fit the model on dataset. Also set `epochs` param to, let's say 100 and set `verbose` to 2\n",
    "                                        # Hint: depending on your dataset you might want to increase the amount of epochs\n",
    "                                        # In case of Bitcoin dataset, it took me 400-500 epochs to see some meaningful results\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# When implemented, you should see that training is started:\n",
    "### Epoch 1/100\n",
    "### 61/61 - 0s - loss: 0.0134\n",
    "### Epoch 2/100\n",
    "### 61/61 - 0s - loss: 0.0133\n",
    "### .........\n",
    "\n",
    "# After training is done, you should the Loss graph displayed with loss value (hopefully) going down with training epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forecasting and errors\n",
    "Congratulations! You've implemented a simple NN that predicts the price of the asset of your choice!\n",
    "\n",
    "Now let's gather the forecast for the whole series, plot it and see what MSE/MAE values we have achieved.\n",
    "\n",
    "Note: gathering the forecast takes some time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "    series_window = series[time:time + window_size]\n",
    "    series_window_expanded = series_window[np.newaxis]\n",
    "    predicted_value = model.predict(series_window_expanded)\n",
    "    forecast.append(predicted_value)\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "# Get back the absolute results to plot the prices and calculate mean errors\n",
    "results_abs = results * max_valid\n",
    "x_valid_abs = x_valid * max_valid\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid_abs)\n",
    "plot_series(time_valid, results_abs, title=\"Deep Learning prediction\", legend=[\"Actual\", \"Forecast\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"MSE:\")\n",
    "print(tf.keras.metrics.mean_squared_error(x_valid_abs, results_abs).numpy())\n",
    "print(\"MAE:\")\n",
    "print(tf.keras.metrics.mean_absolute_error(x_valid_abs, results_abs).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}